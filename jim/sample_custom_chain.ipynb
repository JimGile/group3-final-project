{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jim\\AppData\\Local\\Temp\\ipykernel_28200\\3242483828.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n"
     ]
    }
   ],
   "source": [
    "# Define the model to use\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the model to use\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"I want information on getting a compost bin. I have submitted case number: 9578014.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the individual chains that make up each step of the complex workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the sentiment_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': 'I want information on getting a compost bin. I have submitted case number: 9578014.', 'sentiment': 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "# Define a sentiment prompt that returns a sentiment response\n",
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=[\"email\"],\n",
    "    output_key=\"sentiment\",\n",
    "    template=\"\"\"\n",
    "    Given the following email text:\n",
    "    {email}\n",
    "\n",
    "    Please provide a sentiment analysis with a response value of 'Negative', 'Neutral', or 'Positive'.\n",
    "\n",
    "    Format:\n",
    "    <your sentiment here>\n",
    "    \"\"\"\n",
    ")\n",
    "sentiment_chain = LLMChain(llm=llm, prompt=sentiment_prompt, output_key=\"sentiment\")\n",
    "# sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test the sentiment step of the chain\n",
    "result = sentiment_chain.invoke({\"email\": email})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the topic_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topics and their descriptions\n",
    "topics_dict = {\n",
    "    'Homeless': 'words similar to homeless, shelter, encampment',\n",
    "    'Graffiti': 'words similar to graffiti, paint, tagging',\n",
    "    'Pothole': 'words similar to pothole, holes',\n",
    "    'Animal': 'words similar to animal, pest, pets, barking',\n",
    "    'Vegitation': 'words similar to weeds, trees, limbs, overgrown',\n",
    "    'Neighborhood': 'words similar to HOA, RNO, sidewalk, fence',\n",
    "    'Snow Removal': 'words similar to snow, ice, plows',\n",
    "    'Vehicle': 'words similar to vehicle, car, motorcycle, automobile',\n",
    "    'Parking': 'words similar to parking',\n",
    "    'Police': 'words similar to police, gang, loud, drugs, crime',\n",
    "    'Fireworks': 'words similar to fireworks',\n",
    "    'Dumping': 'words similar to dumping',\n",
    "    'Trash': 'words similar to trash, garbage, compost, recycling',\n",
    "    'Housing': 'words similar to rent, rental, apartments, housing',\n",
    "    'Policy': 'words similar to policy, tax, taxes, mayor, council, councilwoman, councilman, environmental, environment, rezoning, rezone, government, politics',\n",
    "    'Street Racing': 'words similar to racing',\n",
    "    'Transit': 'words similar to transit, traffic, pedestrian, intersection, bicycle, bike, speed, pavement',\n",
    "    'Parks': 'words similar to park, playground, trails, pool, gym, medians',\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a string\n",
    "topic_descriptions = \"\\n\".join([f\"{key}: {desc}\" for key, desc in topics_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "topic_prompt = PromptTemplate(\n",
    "    input_variables=[\"email\", \"sentiment\"],\n",
    "    output_key=\"topics\",\n",
    "    template=f\"\"\"\n",
    "    You are an email classification assistant. Based on the content of the email, please classify it into one or more of the following topics:\n",
    "\n",
    "    {topic_descriptions}\n",
    "\n",
    "    Email content:\n",
    "    {{email}}\n",
    "\n",
    "    Please list the relevant topics based on the email content. If multiple topics apply, separate them with commas. Only return the topic names.\n",
    "    If you do not know the topic, return 'Other'.\n",
    "\n",
    "    Format:\n",
    "    <your topics here>\n",
    "    \"\"\"\n",
    ")\n",
    "topic_chain = LLMChain(llm=llm, prompt=topic_prompt, output_key=\"topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': 'Trash',\n",
       " 'fun_fact': 'Denver is home to a unique program called \"Compost Denver,\" which offers a curbside composting service for residents. This initiative helps divert organic waste from landfills, turning it into nutrient-rich compost for local gardens and farms.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun_fact_prompt = PromptTemplate(\n",
    "    input_variables=[\"topics\"],\n",
    "    output_key=\"fun_fact\",\n",
    "    template=\"Please find a random fun fact about the city of Denver related to any one of the following topics:\\n\\n{topics}\\n\\nKeep your response short and to the point.\")\n",
    "fun_fact_chain = LLMChain(llm=llm, prompt=fun_fact_prompt, output_key=\"fun_fact\")\n",
    "topics =  \"Trash\"\n",
    "result_4 = fun_fact_chain.invoke(topics)\n",
    "result_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': 'I want information on getting a compost bin. I have submitted case number: 9578014.', 'topics': 'Trash, Compost', 'fun_fact': \"Denver's composting program is one of the largest in the nation, diverting over 30% of the city's waste from landfills by collecting organic materials, including food scraps and yard waste, for composting.\"}\n"
     ]
    }
   ],
   "source": [
    "# Test the topic and fun fact chains\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[topic_chain, fun_fact_chain],\n",
    "    input_variables=[\"email\"],  # Initial input needed\n",
    "    output_variables=[\"topics\", \"fun_fact\"]  # Final output of the chain\n",
    ")\n",
    "result_2 = sequential_chain.invoke(email)\n",
    "print(result_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the rag_response_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG response prompt\n",
    "\n",
    "TEMPLATE_TEXT_D4_SPECIFIC = \"\"\"\n",
    "You are an assistant for question-answering tasks specifically for generating responses to constituent emails.\n",
    "You work as a senior aide to Councilwoman Diana Romero Campbell, a Denver City Council member for District 4.\n",
    "You represent the South East Region of the city and county of Denver Colorado USA.\n",
    "Use the following pieces of retrieved context to help answer the question and generate a response to the constituent.\n",
    "If you don't know the answer, just say that you don't know but we will get the information and get back to you.\n",
    "Use three to four sentences maximum, keep the answer concise, and be specific to the city and county of Denver.\n",
    "In your response, please include a fun fact about the city of Denver.\n",
    "\n",
    "Question: {email}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_response_prompt = PromptTemplate(\n",
    "    input_variables=[\"email\", \"context\"],\n",
    "    output_key=\"rag_response\",\n",
    "    template=TEMPLATE_TEXT_D4_SPECIFIC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the embeddings and directory where the vector store database is located\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "persist_directory = \"../chroma_db\"\n",
    "vector_store = Chroma(embedding_function=embedding_function, persist_directory=persist_directory)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Function to format the retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the rag_response_chain\n",
    "rag_response_chain = (\n",
    "    {\n",
    "        \"email\": RunnablePassthrough(),\n",
    "        \"context\": retriever | format_docs, \n",
    "    }\n",
    "    | rag_response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Good afternoon, \\n\\nThank you for your inquiry about obtaining a compost bin. Unfortunately, Denver's rollout of compost service is scheduled district by district, and District 4 is not set to receive compost bins until 2025. I understand the frustration with this delay and assure you that your feedback is crucial as we work with Solid Waste for better communication. Did you know that Denver is known as the Mile High City because its elevation is exactly one mile above sea level? \\n\\nBest regards,  \\n[Your Name]  \\nSenior Aide to Councilwoman Diana Romero Campbell  \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3 = rag_response_chain.invoke(email)\n",
    "result_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "- First Chain (rag_response_prompt): This generates the initial RAG response using retriever, format_docs, and llm.\n",
    "\n",
    "- Summary Step:\n",
    "    The summary_prompt template is fed with the initial RAG response.\n",
    "    This prompt is passed to llm, generating a summary.\n",
    "\n",
    "- Insights Step:\n",
    "    Similar to the summary step, the insights_prompt template takes the initial RAG response.\n",
    "    This output is then passed to the LLM for insights extraction.\n",
    "\n",
    "Notes\n",
    "\n",
    "- RunnablePassthrough: Used to pass intermediate outputs directly into new prompts.\n",
    "- StrOutputParser: Ensures the LLM outputs are parsed correctly and can be chained into subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From chat gpt:\n",
    "# from langchain.schema import RunnablePassthrough, StrOutputParser\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# # Initialize the language model\n",
    "# llm = OpenAI()\n",
    "\n",
    "# # Define other components\n",
    "# retriever = ...  # Your retriever object, like a VectorStoreRetriever\n",
    "# format_docs = ...  # Your document formatting function or chain\n",
    "\n",
    "# Define the RAG response prompt\n",
    "rag_response_prompt = PromptTemplate(template=\"Answer the question based on the following context:\\n\\n{context}\\n\\nEmail: {email}\\n\\nResponse:\")\n",
    "\n",
    "# Additional prompts for subsequent LLM calls\n",
    "summary_prompt = PromptTemplate(template=\"Summarize the following response:\\n\\n{response}\\n\\nSummary:\")\n",
    "insights_prompt = PromptTemplate(template=\"Extract key insights from the following response:\\n\\n{response}\\n\\nInsights:\")\n",
    "\n",
    "# Define the RAG chain with added steps for summarization and insights\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"email\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  # Initial response output\n",
    "    | (  # Add more LLM calls here\n",
    "        {\n",
    "            \"response\": RunnablePassthrough()  # Pass the output from the previous step\n",
    "        }\n",
    "        | summary_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    | (  # Another LLM call for key insights\n",
    "        {\n",
    "            \"response\": RunnablePassthrough()\n",
    "        }\n",
    "        | insights_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Example inputs\n",
    "inputs = {\n",
    "    \"context\": \"Information about AI in healthcare.\",\n",
    "    \"email\": \"Could you provide insights into how AI is used in healthcare?\"\n",
    "}\n",
    "\n",
    "# Run the chain\n",
    "result = rag_chain.invoke(inputs)\n",
    "\n",
    "# Access results from the chain\n",
    "print(\"RAG Response:\", result.get(\"response\"))\n",
    "print(\"Summary:\", result.get(\"summary\"))\n",
    "print(\"Insights:\", result.get(\"insights\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the general_response_chain to search the denvergov.org/Government website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the specific_311_topic_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the fun_fact_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trash'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Denver, the city has an innovative waste management program that includes a \"winner\" in their trash collection system called \"Waste Diversion.\" Denver aims to divert at least 34% of its waste from landfills by encouraging recycling and composting. One fun fact is that the city has a special program that promotes the use of \"green bins\" for composting organic waste, which includes food scraps and yard waste. This initiative not only helps reduce landfill waste but also supports the city’s goal of becoming a more sustainable and environmentally friendly urban area.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun_fact_prompt = PromptTemplate(template=\"Please find a random fun fact about the city of Denver related to any one of the following topics:\\n\\n{topics}\")\n",
    "fun_fact_chain = fun_fact_prompt | llm | StrOutputParser()\n",
    "topics =  result_2['topics']\n",
    "result_4 = fun_fact_chain.invoke(topics)\n",
    "result_4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
